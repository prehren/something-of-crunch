---
title: "Measuring Conceptual Change in Philosophy: A Corpus Study"
date: 2025-01-01
---

Last year, together with two collaborators (Lucien Baumgartner; Krzysztof S&#281;kowski), I worked on a project that aimed to track the semantic disruptiveness of influential philosophy publications. We presented the material a couple of times, but ultimately decided that the project was not worth pursuing. Still, I thought the research was interesting enough methodologically to merit some sort of write up. That&#8217;s what this post is (though I&#8217;m mostly going to focus on the parts that I contributed).

The motivation for the project had to do with <em>conceptual engineering </em>(CE). I have to confess that even though I&#8217;ve now been to more than one workshop on CE, I&#8217;m still not entirely sure what CE is all about, so here&#8217;s a quote from Chalmers (2020): &#8220;Conceptual engineering is the design, implementation, and evaluation of concepts. Conceptual engineering includes or should include de novo conceptual engineering (designing a new concept) as well as conceptual re-engineering (fixing an old concept)&#8221; (1). Some philosophers think CE is a new thing; others disagree. One open question question is whether there are examples of articles or books in the philosophical literature that have caused significant conceptual change in the philosophical community (or parts of it) in the past. In addition to being open, the question is also an empirical one, and so we thought we&#8217;d attempt to address it using the tools of corpus linguistics. We chose three candidate concepts that plausibly underwent change in the recent history of philosophy: <em>belief </em>; <em>consciousness</em>; and <em>reference</em>. We chose these three examples because each features prominently in a very influential book or article published during the last 50 years that many see as having done much to change the concept in question (at least to some extent). For belief: Clark, A., &amp; Chalmers, D. (1998). &#8220;The Extended Mind.&#8221; <em>Analysis</em>, 58(1), 7&#8211;19; for consciousness: Chalmers, D. (1996). <em>The Conscious Mind: In Search of a Fundamental Theory</em>. Oxford University Press; for reference: Kripke, S. (1980). <em>Naming and Necessity</em>. Harvard University Press.

While major problems still remain with testing for conceptual change directly (e.g., Recchia et al., 2017; Hengchen et al., 2021), conceptual change tends to (though need not) go hand in hand with semantic change. This is useful because there are a number of well-established methods to detect and measure semantic change (for a survey, see, Tahmasebi et al., 2019). We chose to use (neural) word embeddings with temporal referencing (Dubossarsky et al., 2019); I&#8217;ll explain what that means in a bit.

To construct our corpus, I used a Python script to download the pdfs of all articles published in six high-profile (see, Leiter, 2022), generalist philosophy journals (<em>Australasian Journal of</em> <em>Philosophy</em>; <em>Analysis</em>; <em>Mind</em>; <em>No√ªs</em>; <em>Philosophical Studies</em>; <em>Synthese</em>) between 1971 and 2021. We only included full-length, original articles (so no abstracts, book reviews, errata, etc.). I then removed cover pages, headers and footers, converted the pdfs to plain text and cleaned up the resulting text files. Table <a href="#x1-8r1">1</a> shows the number of articles from each journal, as well as the total number of articles.

TABLE HERE

Next, we constructed three sub-corpora, one for each target publication <em>T</em>. We reasoned that we should not expect semantic changes to show up across all of philosophy, just in articles on topics related to that of <em>T</em>. To define the limits of these sub-corpora, I used two restrictions. The first restriction is based on PhilPapers categories. PhilPapers is a &#8220;comprehensive index and bibliography of philosophy maintained by the community of philosophers.&#8221; In addition to lots of other useful meta-data, PhilPapers sorts many of its entries into one or more of &#8220;the traditional fields of philosophy&#8221; (both, https://philpapers.org/help/categorization.html; accessed Sept 11, 2023)&#8211;philosophy of mind, normative ethics, philosophy of physics, Asian philosophy, 17th/18th century philosophy, etc. 81.4% of articles in our dataset had at least one field listed. For each <em>T</em>, I excluded all articles that did not share at least one field in common with <em>T</em>.

While a good start, this first restriction alone would likely have been too permissive. Consider Chalmers (1996), which is categorized as Philosophy of Cognitive Science and Philosophy of Mind. Most of the articles published in these two fields are not on belief, and so to keep them all in the sub-corpus would plausibly have muddied the waters. I therefore introduced a second restriction. For each of the three sub-corpora (including <em>T</em>; restricted to nouns), I used LDA (with 2000 Gibbs samples) to identify major topics. As the threshold probability, I more or less arbitrarily picked 0<em>.</em>1 (this resulted in 2 major topics for each of our target publications): If the posterior probability of a topic in a article was at least 0<em>.</em>1, then I considered that topic to be one of the major topics of that article. Figure <a href="#x1-18r1">1</a> shows the results. I then restricted each sub-corpus to articles that shared at least one major topic with <em>T</em>. This left us with 384 texts for Kripke (1980) (2861310 words); 281 texts for Chalmers (1996) (2652613 words); and 498 texts for Clark and Chalmers (1998) (4226818 words).

FIGURE HERE

To create embeddings of our target words (<em>w</em><sub>T</sub>s), we used two common methods, <em>word2vec </em>(Mikolov et al., 2013) and <em>BERT </em>(Devlin et al., 2019).<span class="footnote-mark"><a href="Blog_post_20242.html#fn1x0"><sup class="textsuperscript">1</sup></a></span> word2vec creates static embeddings (for each word, the model creates a single, context-independent vector), while embeddings created with BERT are context sensitive (the same word can get multiple different embedding vectors depending on its context in the text), and so we thought it might be nice to compare the two. I worked with word2vec, and so that&#8217;s what I&#8217;ll focus on. The process for BERT was similar, however, and I&#8217;ll mention how the BERT embeddings compared to the word2vec embeddings. 

To prepare our sub-corpora for training, I removed all stop words, words not in the English dictionary and single letter words. I also lemmatized and POS-tagged all texts. This was done because we were only interested in our target words as nouns, yet one of them (reference) can also be a verb.

Recall that our goal was to investigate if the meaning of any of our three target words has changed perceptibly since 1971 in relevant parts of the philosophical literature. To do so, we used temporal referencing (Dubossarsky et al., 2019). The idea is simple: First, label your target word(s) with their time information; then, train a global model on the entire corpus. Since in our case, there were generally many articles per year, we decided to label target words not just with their year, but also with the article ID. So, for example, occurrences of &#8216;belief//NOUN&#8217; in Clark and Chalmers (1998) became &#8216;belief//NOUN_Analysis_1998_58_1_7-19&#8217;. For each of our three sub-corpora, I then trained a SGNS model (dim = 300, window = 10). To investigate semantic change, we traced the average pairwise cosine similarity over time. For years <em>y </em>and <em>y </em>+ 1, I calculated the pairwise cosine similarity between each of <em>w</em><sub>T</sub>&#8217;s embeddings in texts published in <em>y </em>(one for each text since word2vec generates static embeddings) and each of <em>w</em><sub>T</sub>&#8217;s embeddings in texts published in <em>y </em>+ 1, and then averaged these values. Figure <a href="#x1-18r1">1</a> shows the results.

FIGURE HERE

Are there identifiable points at which the meaning of our <em>w</em><sub>T</sub>s has changed? To test for this, we used change-point detection. There are many methods to look for change-points in time series data (for a survey, see, Aminikhanghahi and Cook, 2017); here, we used kernel-based change point analysis with a linear kernel. The basic idea with this method is to average all the values in the time series within a certain window of time immediately before and after a given candidate change-point, and then to check if the difference between these two means exceeds a certain threshold. If it does, a (local) change-point has been detected. Table <a href="#x1-27r2">2</a> shows the results, for the word2vec embeddings as well as the BERT embeddings. We find that the only <em>w</em><sub>T</sub> for which there is evidence of the presence of local change-points across both models is &#8216;consciousness.&#8217; (Lucien had BERT generate two different kinds of embedding, narrow word embeddings, for which the model considers a relatively small window of tokens around each word, and broad usage embeddings, for which the model considers a much larger window of tokens around each word [in this case, 128]).

Next, we wanted to investigate the possible role of our target texts. Recall that we chose our three <em>w</em><sub>T</sub>s because each features prominently in a very influential book or article published during the last 50 years that many see as having done much to change the concept in question. To do so, we relied on the idea of a text&#8217;s <em>disruptiveness </em>with respect to some word&#8217;s meaning: when a text <em>T </em>is disruptive with respect to a given word&#8217;s meaning, then that word&#8217;s meaning will be tend to be more similar to its meaning in texts published after <em>T </em>(because <em>T </em>had an influence on these texts) than to texts published before <em>T</em> (see, Hogenbirk, 2023). To quantify disruptiveness, we defined a disruptiveness score <em>D</em>:

$$D_{T,w} = \frac{1}{n_{\text{after}}}\sum_{t in y_T+o_{\text{after}}<y_t}$$

with <em>y</em><sub>i</sub> the year of text <em>i</em>&#8217;s publication, <em>o</em><sub>before</sub> and <em>o</em><sub>after</sub> offsets (how many years immediately before/after <em>T </em>are not to be included in the calculation), <em>r </em>the time window [how many years before/after <em>T </em>(minus/plus the offset) to include in the calculation], and <em>n</em><sub>before</sub> and <em>n</em><sub>after</sub> the numbers of text published in <em>r </em>before/after <em>T</em>. The interpretation of this score is that if <em>D &#x003E; </em>0, then the target text <em>T </em>was more similar to texts published after it than to texts before it (with respect to the meaning of <em>w</em>), and so it was disruptive. Figure <a href="#x1-29r3">3</a> shows <em>D</em>-score rankings for our three sub-corpora. None of the three target texts rank near the top; in fact, their <em>D</em>-scores are all negative. (The BERT embeddings yielded similar results.) Hence, we did not find evidence that any of our targets text were unusually disruptive with respect to their target words.

FIGURE HERE

<em>D</em>-scores need not be constant over time. This means that even though our target texts were not unusually disruptive globally, they may still have been unusually disruptive for their time. To investigate this possibility, for each sub-corpus, I calculated the average <em>D</em>-score for each year and compared it with our target&#8217;s <em>D</em>-score (Figure <a href="#x1-33r4">4</a>). For Clark and Chalmers (1998) and Kripke (1980), we see little evidence for <em>D</em>-score fluctuations over time. In contrast, in the case of Chalmers (1996), Figure <a href="#x1-33r4">4</a> shows a spike in the average disruptiveness of articles with respect to consciousness around the time of that text&#8217;s publication. Note, however, that we did not find this spike for the BERT embeddings; moreover, the 95% confidence intervals are such that our results are also consistent with no spike at all.

FIGURE HERE

So far, I have used <em>o</em><sub>before</sub> = <em>o</em><sub>after</sub> = 0 and <em>r </em>= 10. Perhaps it takes longer than 10 years for a text&#8217;s disruptiveness to be felt in the relevant literature. To investigate this, I re-calculated the <em>D</em>-scores using a range of different offsets <em>o</em><sub>after</sub>. Note that the maximum possible value of <em>o</em><sub>after</sub> depends on a text&#8217;s year of publication. Figure <a href="#x1-34r5">5</a> shows the results. The figure suggests that the choice of <em>o</em><sub>after</sub> does not have a major impact. 

FIGURE HERE

Are there examples of articles or books in the philosophical literature that have caused significant conceptual change in the recent history of analytic philosophy? Of the three candidate examples we investigated (belief; consciousness; reference), only consciousness showed robust evidence of semantic change. After mapping the semantic similarity of &#8216;consciousness&#8217; in a suitable corpus of philosophy articles published since 1971 over time, we detected the presence of local change-points for embeddings generated using two different methods (word2vec, BERT). However, even though many of these change-points lay roughly in the neighbourhood of the publication of Chalmers (1996), the results of our <em>D</em>-score analysis do not suggest that Chalmers (1996) itself was unusually disruptive with respect to the meaning of &#8216;consciousness&#8217;. Our results, then, do not support the idea that any of our three target texts caused substantial conceptual change in the philosophical literature.

To end, I want to briefly describe two of the main problems that ultimately lead me to not want to pursue the project further. The first is practical. Our corpus consisted of articles published in or after 1971. This meant that we were limited to target texts published after 1971. Yet most of the target texts that we (and others) thought worth investigating (e.g., Tarski, 1936; Gettier, 1963; Carnap, 1936) were published (long) before 1971. In addition, two of our target text were published in the second half of the 1990s, meaning that we only had <span class="lmsy-10x-x-109">~</span>25 years worth of data to investigate&#8212;conceptual change might take longer than that, however. The problem is that the further back into the past one wants to go, the more difficult and labour-intensive it becomes to assemble a corpus of sufficient quality: generally, the older a document, the more annoying it is going to be to convert it into clean plain text.

The second problem is a fundamental methodological problem. To be persuaded that our methods do what we want them to do, we&#8217;d (minimally) need a set of examples for which we are <em>reasonably certain </em>that they have undergone conceptual change to calibrate these methods against. While some studies have attempted this sort of calibration in the past (e.g., Martinc et al., 2020), our corpus consisted of philosophy publications&#8212;not a discipline particularly known for its plain or ordinary use of English. This makes it doubtful that these previous results (which are, in my opinion, not that overwhelming to begin with) generalize to our corpus. Instead, what we would have needed are clear-cut examples of conceptual change <em>specifically </em>in the philosophical literature (ideally, in the last 50 years or so). Yet the sense I got while working on this project is that such examples might be impossible to find. When we presented the material, I cannot remember a single example of a concept for which everyone in the room agreed that it had undergone substantial change: not among our chosen examples, nor among any of the many alternative examples people suggested to us. In light of this, it felt a bit pointless to pour lots more time and effort into the project, given that fundamental methodological doubts would likely have always remained.
